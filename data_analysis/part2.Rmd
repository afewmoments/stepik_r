---
title: "data_analysis"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Этап 2

## Используемые библиотеки

```{r}
library(tidyverse)
library(data.table)
library(plotly)
```

## Функции семейства apply

### Основы

Возьмём встроенный набор данных

```{r}
data(diamonds)
str(diamonds)
```

Посмотрим минимальный размерность для каждого бриллианта

Через цикл:

```{r}
min_size <- c()

for (i in 1:nrow(diamonds)) {
  min_size <- c(min_size, min(diamonds[i, 8:10]))
}

head(min_size)
```

Это очень долго, в силу особенностей языка R. Можно ускорить, если сообщить какой тип данных будет у вектора, и сколько там будет значений. На каждом этапе сейчас мы перезаписываем.

```{r}
min_size <- c(nrow(diamonds))

for (i in 1:nrow(diamonds)) {
  min_size[i] <- min(diamonds[i, 8:10])
}
```

Теперь разберём правильный код. На всём протяжении работы с данными, мы будем применять к данным какие-то функции. Поэтому apply становится незаменимым фреймворком

```{r}
min_size <- apply(diamonds[, 8:10], 1, min) # 1 -- работая со строчками
```

Задача решена в одну строчку и очень быстро.

### Как работает

```{r}
apply(X, margin, ...)
```

X --- данные

MARGIN --- как рименяем функцию, по строчкам или по столбцам

FUN --- какая функция применяется

Потренируемся.

```{r}
d <- matrix(rnorm(30), nrow = 5)

apply(d, 1, sd)
```

apply взяла первую строчку данных, применила к ним sd. Потом вторую, и т.д. до конца.


```{r}
apply(d, 2, sd)
```

Всё тоже самое --- только к колнкам

#### Задача

В переменной d сохранен dataframe с произвольным числом количественных переменных. При помощи функции apply найдите максимальное значение в каждой строке. Сохраните результат (вектор максимальных значений) в переменную row_max.

```{r}
row_max <- apply(d, 1, max)
```

### Как работает, продолжение

Если функция возвращает одно значение по результатам работы, то мы получаем вектор длинной по кол-ву строк или столбцов. Но что если функция возврщает несколько значений. Допустим функция range --- максимум и минимум

```{r}
my_range <- apply(d, 2, range)
```

Мы получаем не вектор, а матрицу.

А что если функция возвращает разное кол-во данных. Например Функция которая выводит только выбросы

```{r}
outliers_count <- function(x){
  outliers <- x[abs(x - mean(x)) > 2 * sd(x)]
  if (length(outliers) > 0) {
    return(outliers)
  } else {
    return("There are no outliers")
  }
}
```

Посмотрим есть ли выбросы в данных iris

```{r}
iris_num <-  iris[, 1:4]

apply(iris_num, 2, outliers_count)
```

Так как длины разные, то выходная запись --- лист с данными.

Apply --- инструмент, который хорошо заменяет цикл for.

### Многоточие

Через многоточие можно передавать аргументы используемой функции

```{r}
head(airquality)

apply(airquality, 2, mean)
```

Работает вот так

```{r}
apply(airquality, 2, mean, na.rm = T)
```

### Компактная функция --- можно прописывать функцию в теле apply

```{r}
set.seed(42)

d <- as.data.frame(matrix(rnorm(30), nrow = 5))

my_fun <- function(x) x * 2

```

Такая функция нигде не засветится.

Внутри функции apply можно записывать функцию. Это короткая функция

```{r}
apply(d, 2, function(x) x[x < 0])
```

#### Задача

Давайте завершим и слегка модифицируем задачу из предыдущей лекции. Напишите функцию get_negative_values, которая получает на вход dataframe произвольного размера. Функция должна для каждой переменной в данных проверять, есть ли в ней отрицательные значения. Если в переменной отрицательных значений нет, то эта переменная нас не интересует, для всех переменных, в которых есть отрицательные значения мы сохраним их в виде списка или матрицы, если число элементов будет одинаковым в каждой переменной (смотри пример работы функции).

```{r}
test_data <- as.data.frame(list(V1 = c(NA, -0.5, -0.7, -8), V2 = c(-0.3, NA, -2, -1.2), V3 = c(1, 2, 3, NA)))

get_negative_values <- function(test_data){    
negative_col <- apply(test_data, 2, function(x) any(x[!is.na(x)] < 0))    
return(apply(test_data[negative_col], 2, function(x) x[!is.na(x) & x <0]))}
```

### Продолжение

Можно было бы применить дисперсионный анализ

```{r}
head(iris)
aov(Sepal.Length ~ Species, data = iris)
```

Но это делается только по одной переменной, давайте сделаем это для всех переменных
Берем только количественные перменные

```{r}
aov_result <- apply(iris[, 1:4], 2, function(x) aov(x ~ iris$Species))

norm_test <- apply(iris[, 1:4], 2, 
                   function(x) shapiro.test(x))


# Сразу достаём p.value
norm_test_p <- apply(iris[, 1:4], 2, 
                   function(x) shapiro.test(x)$p.value)

```

Этот код сделает сравнение трёх групп между собой по всем количественным перменным

Применяем статистический тест к нескольким параметрам

Для второй части по статистике --- p-уровень значимость

#### Задача

Напишите функцию na_rm которая заменяет все пропущенные значения в столбцах dataframe на соответствующее среднее значение. То есть все NA в первом столбце заменяются на среднее значение первого столбца (рассчитанного без учета NA). Все NA второго столбца заменяются на среднее значение второго столбца и т.д.  Замена пропущенных значений на среднее. change NA to mean

Функция na_rm на вход получает dataframe произвольной размерности, состоящий из количественных переменных. Функция должна возвращать  dataframe с замененными NA. Ни порядок столбцов, ни порядок строк в dataframe изменять не нужно.

Вы можете создавать вспомогательные функции для решения этой задачи. 

```{r}
test_data <- as.data.frame(list(V1 = c(NA, NA, NA, NA, 13, 12, 9, 10, 8, 9, 11, 11, 10, 12, 9), V2 = c(NA, 12, 8, NA, 11, 11, 9, 8, 8, 10, 10, 11, 10, 10, 10), V3 = c(NA, 5, NA, 13, 12, 11, 11, 14, 8, 12, 8, 8, 10, 10, 8), V4 = c(10, 10, 10, 10, 13, 10, 11, 7, 12, 10, 7, 10, 13, 10, 9)))


na_rm <- function(x){
  result <- apply(x, 2, function(y){
  y[is.na(y)] <- mean(y, na.rm=T)
  return(y)
  })
  return(as.data.frame(result))
}

na_rm(test_data)

# Решение через ifelse

na_rm <- function(x){
  y <- apply(x,2,function(x) ifelse(is.na(x),mean(x,na.rm=T),x))
  return(as.data.frame(y))}

```

### lapply, sapply, tapply, by, vapply, mapply

В чём основная идея --- применяем некоторую функцию к объекту.

Почему их много? Потому что у apply  есть особенности:

* apply  нельзя применить к вектору, или дата.фрейму, списку

Для списков есть lapply

```{r}
my_list <- list(x = c(rnorm(30), NA), y = rnorm(10))
str(my_list)

#  она чувствительна к выбросам
lapply(my_list, mean) 

#  но можно так же через ... протаскивать аргументы функции
lapply(my_list, mean, na.rm = T)

#  основные идеи такие же --- можно писать функцию
lapply(my_list, function(x) x * 2)
```

Отличие 

* lapply всегда возвращает список той же длинны что и на входе.

* sapply упрощает вывод, пытается вернуть либо в виде вектора, либо в виде матрицы. И только если не получилось, возвращает в виде списка

```{r}
sapply(my_list, range, na.rm = T)
sapply(my_list, range, na.rm = T, simplify = F)
```

#### Задача

Напишите функцию positive_sum, которая получает на вход dataframe с произвольным количеством числовых переменных. Основная задача функции - найти сумму положительных значений в каждой переменной и сохранить их в список. Рассмотрим пример работы функции на небольшом примере:

Моё решение:

```{r}
positive_sum <- function(d){
  d <- apply(d, 2, function(x)(ifelse(x > 0, x, 0)))
  d <- as.data.frame(d)
  lapply(d, sum, na.rm = T)
}

positive_sum(d)
```

Гораздо более изящное решение, суммирование по индексам:

```{r}
positive_sum <- function(d) {
  lapply(d, function(x) sum(x[x>0], na.rm = T))
  }
```

### Практическая особенность sapply

Эти функции можно применять к вектору.

Есть задача, есть вектор с именами cars и есть какая-то машина с точным названием. Мы хотим понять, есть ли какая-то машина в векторе cars с таким же названием как и искомая машина

```{r}
cars <- c("Mazda", "Volga", "Merc")
car <- c("Mazda RX4")
```

Просто поэлементное сравнение ничего не даст. Но есть функция grepl(). Она проверяет, содержится ли первый вектор во втором

```{r}
grepl("Mazda", "Mazda RX4")
```

Наша задача понять является этот элемент 

```{r}
sapply(cars, function(x) grepl(x, car))
```

Так как это логический вектор, его можно использовать в качестве индексации

```{r}
cars[sapply(cars, function(x) grepl(x, car))]
```

Обратите внимание на следующее выражение, которое очень часто будет вам помогать при работе с данными:

давайте напишем команду, которая отбирает только количественные колонки в данных:

```{r}
iris_num <- iris[sapply(iris, is.numeric)]
```

Готово! sapply(iris, is.numeric) возвращает вектор логических значений, который мы и используем для индексации.

Этот пример также иллюстрирует идею, что lapply и sapply можно применять к dataframe. Так как dataframe - это в том числе и список.

Например, результат команды:

```{r}
sapply(iris[1:4], sd)
```

эквивалентна результату: 

```{r}
apply(iris[1:4], 2, sd)
```

так как каждая колонка dataframe - это и есть элемент списка, то функция lapply и sapply возвращает результат применения некоторой функции к каждой колонке данных!
Но тут есть одно но!

Как вы помните, apply производит все опперации именно над матрицами, поэтому если вы отправите в apply dataframe с разными типами данных, то R сначала приведет все колонки к одному типу, чтобы получилась матрица, т.к. в матрице могут храниться данные только одного типа! Это в свою очередь может привести к неожиданному результату:

```{r}
sapply(iris, is.numeric)

apply(iris, 2, is.numeric)
```

По результатам команды apply можно подумать, что в данных нет количественных переменных! Дело в том, что перед тем как применить функцию is.numeric, сначала данные iris были переведены в матрицу, а все переменные переведены в строки, как в наиболее общий тип данных. В результате получаем для каждой колонки FALSE.

Вот такой вот тонкий момент, о котором нужно помнить, применяя функцию apply к data.frame. В свою очередь с sapply и lapply такого не случится, т.к. в этом случае мы по очереди применим требуемую функцию к каждой колонке данных, как к каждому элементу списка! 

### Функция tapply. Спойлер --- aggregate лучше

Они не так встречаются. На вход принимает вектор, группирует по какому-то индексу, и применяет к этим группам функцию

```{r}
tapply(mtcars$mpg, mtcars$am, mean)
```

У этой функции есть более удобный аналог, функция aggregate

```{r}
aggregate(mpg ~ am, mtcars, mean)
```

Есть ещё функция by, которая группирует переменные

```{r}
by(iris[1:4], iris$Species, colMeans)
```

Как работает эта функция --- она берёт датафрейм ирис, и разбивает на несколько датафреймов в зависимости от перменной Species. Внутри функции by мы должны писать только те функции которые применимы к датафрейму

Можно к примеру написать такую функцию, которая проверяет на нормальность каждую количественную переменную, в зависимости от группирующей переменной.

```{r}
by(iris[1:4], iris$Species, 
   function(x) sapply(x, 
                      function(col) shapiro.test(col)$p.value))
```

Но не стоит на ней заостряться, функционал можно воспроизвести при помощи aggregate

```{r}
aggregate(. ~ Species, iris, function(x) shapiro.test(x)$p.value)
```

### Функция vapply

Такая же как lapply, только мы заранее говорим что хотим получить на выходе

```{r}
vapply(mtcars, mean, FUN.VALUE = numeric(1))
sapply(mtcars, mean)
```

Мы сообщаем на выходе, что хотим получить. Это ускоряет чисто вычислительный процесс

### Функция mapply

Она последовательно берёт элементы двух векторов и посылает их в функцию.

```{r}
mapply(rep, c(1, 2, 3, 4), c(1, 2, 3, 4))

rep(1, 3)
x <- c(20, 25, 13)
m <- c(0, 1, 2)
s <- c(3, 5, 6)
mapply(rnorm, x, m, s)
```


Допустим у нас есть матрица размером 100 на 200:

```{r}
m <- matrix(rnorm(100 * 200), nrow = 100)
```

И мы хотим присвоить имена строчкам и столбикам в этой матрице по принципу:

row_1, row_2, row_3, ..., row_100 - для строк

col_1, col_2, col_3, ..., col_200 - для колонок

Тогда мы могли бы сгенерировать список данными именами следующим образом:

```{r}
m_names <- mapply(paste, list("row", "col"), list(1:100, 1:200), sep = "_")
str(m_names)
```

### Подводный камень

Хотелось бы рассмотреть еще один подводный камень применения функций семейства apply к dataframe.

Предположим, мы решили написать простенькую функцию для расчета стандартного отклонения количественных переменных в данных.

```{r}
get_sd <- function(x){
  num_var <- sapply(x, is.numeric)
  sapply(x[, num_var], sd)
}
```

Казалось бы, все логично и работает на различных примерах:

```{r}
get_sd(iris)
```

Но в нашем коде скрыта серьезная уязвимость!) Предположим, у нас есть набор данных, в котором только одна количественная переменная:

```{r}
my_df <- data.frame(x = 1:10, y = letters[1:10])
get_sd(my_df)
```

Что вообще только что произошло? Дело в том, что существуют различные способы обращения к колонкам dataframe:

my_df[1] - получим dataframe

my_df[[1]] - получим вектор

my_df[, 1] - получим вектор

В случае, если у нас только одна количественная переменная, обращение x[, num_var] вернет колонку в виде вектора, а sapply применит функцию sd к каждому наблюдению вместо того, чтобы применить ко всей переменной.

Таким образом, если вы хотите применить какую-либо функцию к неизвестному заранее числу колонок в данных, лучше используйте такую индексацию типа: my_df[col_index]. То есть:

```{r}
get_sd <- function(x){
  num_var <- sapply(x, is.numeric)
  sapply(x[num_var], sd)}> get_sd(my_df)
```

также можно использовать параметр drop = FALSE при индексации (x[,num_var, drop = FALSE)

### Задача

Предположим у нас есть dataframe с двумя переменными name - название гена, expression - уровень экспрессии. Например:

```{r}
test_data <- as.data.frame(list(name = c("p4@HPS1", "p7@HPS2", "p4@HPS3", "p7@HPS4", "p7@HPS5", "p9@HPS6", "p11@HPS7", "p10@HPS8", "p15@HPS9"), expression = c(118.84, 90.04, 106.6, 104.99, 93.2, 66.84, 90.02, 108.03, 111.83)))

names = c("HPS5", "HPS6", "HPS9", "HPS2", "HPS3", "HPS7", "HPS4", "HPS8")
```

Обратите внимание, что само название гена спрятано внутри строки и указано после символа @. Напишите функцию my_names, которая получает на вход  датафрейм и вектор с именами тех генов, для которых мы хотим отобрать наблюдения уровня экспрессии. Допустим, мы хотим отобрать наблюдения только для генов 'HPS1' и 'GOT1', тогда результат работы функции будет следующий:

выстрадал решение:

```{r}
my_names <- function (dataset, names){
  t <- sapply(names, function(x) which(grepl(x, dataset[, 1])))
  dataset[as.vector(t), ]
}

my_names(test_data, names)
```

Как работает функция:

* grepl(x, dataset[, 1]) --- ищет совпадение задаваемой строки в первом столбце заданных данных и возвращает матрицу, в которой на месте пересечения входа X в строку dataset[, 1] стоит TRUE

* sapply(names, function(x) which(grepl(x, dataset[, 1]))) --- возврщает номера строк TRUE, в которых было пересечение X  с dataset[, 1] 

*  dataset[as.vector(t), ] по индексу берём все строки, удовлетворяющие условию

### Задача

Задачка посерьезнее! Друзья, это задание действительно сложное, если не удается его решить, не печальтесь, пройдите курс дальше, вооружитесь новыми знаниями, например, вот этими или этими, вы всегда можете вернуться к этому заданию! Спойлер: решение всего-то в четыре строчки!)

Напишите функцию find_outliers, которая получает на вход dataframe с одной количественной переменной и произвольным числом факторных переменных. Факторные переменные разбивают все наши наблюдения на определенное число групп. Например, если мы посмотрим на данные mtcars и возьмем в качестве группирующих переменных: am - две градации и cyl три градации, то получим 6 групп наблюдений на пересечении градаций этих переменных. Рассчитаем, к примеру, средние в каждой из шести групп:

```{r}
aggregate(mpg ~ cyl + am, mtcars, mean)

test_data <- read.csv("https://stepic.org/media/attachments/course/724/hard_task.csv")
head(test_data)
str(test_data)

correct_answer <- read.csv("https://stepic.org/media/attachments/course/724/hard_task_ans.csv")
head(correct_answer)
str(correct_answer)

```

Итак, ваша задача — создать в данных новую числовую переменную is_outlier, которая будет принимать значение 1, если наблюдение в этой строке является выбросом в своей группе, и 0, если не является.

Под выбросами будем понимать наблюдения, отклоняющиеся от среднего значения в группе более чем на два стандартных отклонения этой группы. 

Поясню условие на примере данных ToothGrow. Сначала переведем группирующие переменные в фактор. Одна из группирующих переменных имеет  две градации, другая три, значит все наши наблюдения разбиваются на шесть групп. Ваша задача проанализировать каждую группу и отметить там выбросы, если они есть. Иными словами, сначала мы берем все наблюдения для которых supp = VC и dose = 0.5. В этой группе смотрим, есть ли наблюдения, у которых значения len отклоняются от среднего в этой группе больше чем на два sd. И так повторяем для оставшихся трех групп. В итоге в новой переменной is_outlier будет храниться информация - является ли наблюдение выбросом не по всем данным, а только в своей группе.

А также: порядок переменных является случайным, единственная количественная переменная может быть первым столбцом, может последним и т.д.

```{r}
find_outliers <- function(t){
  # ищем колчиественную переменную, и записываем её в var_name
  var_name <- colnames(t[sapply(t, is.numeric)])
  # группируем данные по факторным переменным
  t <- group_by_at(t, vars(one_of(colnames(t[sapply(t, is.factor)]))))
  # вводим переменную is_outlier
  t <- mutate(t, is_outlier = ifelse(abs(get(var_name) - mean(get(var_name))) > 2 * sd(get(var_name)), 1,0)) 
  return(t)
}

ToothGrowth$dose <- factor(ToothGrowth$dose)
find_outliers(ToothGrowth)
```

Данная задачка — не праздное развлечение, чаще всего мы проверяем распределение на нормальность именно в контексте нескольких групп, а не по всей переменной в целом. Наша функция, создав новую переменную, позволит сделать по этой переменной subset и убрать все выбросы:

```{r}
ToothGrowth  <- find_outliers(ToothGrowth)
clear_data <- subset(ToothGrowth, is_outlier == 0)
```

### Задача

Перейдем от этапа предобработки данных к применению уже знакомых нам статистических критериев. Кстати, если вы еще не решали практические задачи из курса Основы статистики. Часть два, рекомендую вам обратить на них внимание. В конце каждого модуля есть подборка практических задач, решение которых, поможет вам закрепить пройденный в этом модуле материал.

Типичная задача, с которой мы сталкиваемся при анализе данных - это исследование большого числа переменных и их взаимосвязей между собой. Умение алгоритмизировать рутинные операции сэкономит вам массу времени. 

Рассмотрим следующую ситуацию: у нас есть dataframe с произвольным числом количественных переменных. Мы хотим построить линейную регрессию для предсказания значений зависимой переменной, однако, в качестве предикторов мы хотим использовать только те переменные, распределение которых значимо не отличается от нормального (p - value теста Шапиро - Уилка больше 0.05).

Напишите функцию smart_lm, которая получает на вход data.frame с произвольным числом количественных переменных. Первая колонка в данных - это зависимая переменная, все остальные - предикторы. На первом этапе вы должны отобрать предикторы для модели. 

Функция возвращает в виде вектора коэффициенты линейной регрессии построенной только для отобранных предикторов (условие нормальности распределения). Если таких предикторов в данных не оказалось, то функция возвращает предупреждение "There are no normal variables in the data".

Перед тем как сдавать код, советую убедиться, что вы прошли такие тесты как:

```{r}
smart_lm <- function(df){
  # Сначала проверяем особые случаи, когда переменных две
  if(length(df) == 2){
    # Если переменные две, то проверяем вторую переменную на нормальность. Если она нормальна, выводим коэффициенты
    # Если нет, то выводим предупреждение
    if(shapiro.test(df[, 2])$p.value > 0.05){
  lm(df[,1] ~ df[,2], df)$coefficients
} else {
  return("There are no normal variables in the data")
}
    # Если переменных больше, то удаляем первую переменную (она будет предикантом)
    } else {
      t <- df[, -1]
      # Записываем во фрейм данных только нормальные переменные
      s <- t[sapply(t, function(x) shapiro.test(x)$p.value) > 0.05]
      # Делаем проверку, если вектор не пустой, значит там есть нормальные переменные
      if(length(s) > 0){
        # Через функцию as.formula записываем формулу, в которой перечисляем все названия нормальных переменных
        fmla <- as.formula(paste("depend ~ ", paste(colnames(s), collapse = "+")))
        #  Добавляем во фрейм данных изначально удалённую зависимую переменную
        s <- mutate(s, depend =  df[, 1])
        # Возвращаем коэффициенты линейной модели
        return(lm(fmla, s)$coefficients)
        # Если вектор пустой, выводим предупреждение
        } else {
          return("There are no normal variables in the data")
      }
    }
  }
```

Вот более элегантное решение. Здесь не вводится формула, её заменяет точка "."

```{r}
smart_lm <- function(x){    
check_norm <- sapply(x[-1], function(var) shapiro.test(var)$p.value > 0.05)    
if (any(check_norm)){    
x = x[, c(1, (which(check_norm) + 1))]    
coef <- lm(x[[1]] ~ ., x[-1])$coef    
return(coef)    
} else{    
return('There are no normal variables in the data')}}
```

### Задача

Иногда возникает необходимость применить какой-либо критерий к большому числу количественных переменных. В этой задаче мы используем выборочный t - test, который сравнивает выборочное среднее с предполагаемым средним в генеральной совокупности.  

Напишите функцию one_sample_t, которая получает на вход два аргумента:

1. Dataframe произвольного размера с произвольным числом переменных различного типа.

2. Числовое значение среднего в генеральной совокупности.

Ваша функция должна применять одновыборочный t - test к каждой числовой переменной в данных, и сравнивать среднее значение этой переменной с указанным значением среднего в генеральной совокупности (второй аргумент функции).

Функция должна возвращать список, где каждый элемент это вектор, состоящий из t - значения, числа степеней свобод (df) и значения p - value.

```{r}
one_sample_t <- function(test_data, general_mean){
  # отбираем только числовые переменные
  numeric_data <- test_data[sapply(test_data, is.numeric)]
  # к числовым переменным применяем t.test и отбираем интересующие параметры
  # выводим списком, как это требуется в задании
  lapply(numeric_data, function(x) c(t.test(x, mu = general_mean)$statistic, 
                                     t.test(x, mu = general_mean)$parameter, 
                                     t.test(x, mu = general_mean)$p.value))
  }
```

Фактически всё тоже самое, но в более элегантной форме:

```{r}
one_sample_t <- function(data, mean) {
    lapply(data[sapply(data, is.numeric)],
           function(x) unlist(t.test(x, mu = mean)[c("statistic", "parameter", "p.value")]))
}
```

### Задача

Итак, ваша задача, написать функцию get_p_value, которая получает на вход список (назовем его главным списком), каждый элемент этого списка тоже список - результат выполнения функции shapiro.test (смотри пример normality_tests). Ваша задача из каждого элемента главного списка вытащить только p - value. В итоге функция возвращает список где каждый элемент - одно значение - p - value (как в примере normality_tests_p).

```{r}
get_p_value <- function(test_list){
  lapply(test_list, function(x) x$p.value)
}
```

## Работа с данными при помощи dplyr

Уже знакомая библиотека, которую мы уже знаем.
 
```{r}
my_data <- data_frame(x = rnorm(10000), y = rnorm(10000), 
                      f = factor(rep(1:2, 5000)))

# а ниже классический дата.фрейм
my.data <- data.frame(x = rnorm(10000), y = rnorm(10000), 
                      f = factor(rep(1:2, 5000)))
```

В чём различия? 

* dplyr --- прерывает излишний вывод, и выводит тип данных

* dplyr --- разрешает называть перменные с пробелами

* dplyr --- разрешает использовать функции внутри функции

```{r}
my_data_2 <- data_frame(x = rnorm(10), y = abs(x))
# my.data.2 <- data.frame(x = rnorm(10), y = abs(x))  не работает
```

### Работа с колонками и строками 

* select --- позволяет отобрать колонки

* slice --- позволяет отобрать строки


```{r}
select(diamonds, 1, 2, 3)
diamonds[c("cut", "price", "color")]

# Можно  обращаться к столбцам которые содержат t
select(diamonds, contains("t"))

# step 6 slice rows
slice(diamonds, c(1, 4, 5))
diamonds[c(1, 4, 5)]
```

###  Функция filter

```{r}
filter(diamonds, carat > 0.3 | color == "J")
diamonds[diamonds$carat > 0.3 & diamonds$color == "J", ]
subset(diamonds, carat > 0.3 & color == "J")
```

###  Функция arrange --- сортировка

```{r}
arrange(diamonds, desc(price))
diamonds[order(diamonds$price, diamonds$depth), ]
```

###  Функция rename и mutate

Тоже уже знакомы

#### Задача

Давайте потренируемся обращаться к данным. Вы можете использовать базовый синтаксис, функции из пакета dplyr или data.table. 

Поработаем с данными diamonds из пакета ggplot2. 

В переменную d сохраните только нeчетные строчки исходных данных diamonds. 

Обратите внимание на функцию seq(). Она может вам пригодиться вам не только в этой задаче.

```{r}
d <- slice(diamonds, seq(1, nrow(diamonds), 2))
```

#### Задача

Потренируемся использовать изученные функции. Из данных mtcars отберите только четыре переменные: mpg, hp, am, vs. 
Оставьте только те наблюдения, для которых значения mpg > 14 и hp > 100. 
Отсортируйте получившиеся данные по убыванию переменной mpg и возьмите только первые 10 строчек. 
Переменную mpg переименуйте в Miles per gallon, а переменную hp в  Gross horsepower (обратите внимание, dplyr позволит нам создать пременные с пробелами в названии). Получившийся dataframe сохраните в переменную my_df. 

```{r}
my_df <- mtcars %>% 
  select(mpg, hp, am, vs) %>% 
  filter(mpg > 14 & hp > 100) %>% 
  arrange(desc(mpg)) %>% 
  slice(1:10) %>% 
  rename("Miles per gallon" = mpg,
         "Gross horsepower" = hp)
mtcars %>% 
  slice(1:10)
```

## Работа с данными при помощи dplyr продолжение

Есть полезная функция mutate_each

```{r}
d <- as_data_frame(matrix(rnorm(30), ncol = 5))

mutate_each(d, funs(ifelse(. < 0, 0, .)))
```

В funs --- точка «.» означает колонку данных
Мы применяем для каждой колонке данных применим функцию. Её преимуществом является то, что она возвращает датафрейм

sample_n выдаёт любые значения

```{r}
gr_diamonds <-  group_by(diamonds, cut)

sample_n(diamonds, 2)
slice(diamonds, 1)

sample_n(gr_diamonds, 2)

slice(gr_diamonds, 1)
```


summarise --- схлопывает значения в колонке в одно значение. Очень полезная и удобная функция.



Сгруппированный датафрэйм выдаёт группировку по каждой группе


#### Задача

Напишите функцию, all_to_factor, которая преобразует dataframe, переводя все его переменные в фактор.

```{r}
all_to_factor <- function(x){
  mutate_each(x, funs(as.factor(.)))
}

all_to_factor(mtcars)
```

#### Задача

В этом задании от вас потребуется написать функцию для предобработки данных log_transform. В статистике часто трансформируют исходные переменные. Например, используют значение натурального логарифма исходной переменной.

Ваша задача написать функцию, которая получает на вход dataframe  с произвольным числом переменных разных типов. На первом этапе функция должна выполнить предобработку числовых переменных. Т.к. значение логарифма мы можем рассчитать только для положительных чисел. Для этого сделаем центрирование всех переменных (Rescaling), только еще добавим единичку, чтобы у нас не осталось нулей:

После того как мы масштабировали каждую переменную, осталось рассчитать значение натурального логарифма каждого наблюдения (функция log) и вернуть новый dataframe. 

```{r}
test_data <- data_frame(V1 = rnorm(5),
                        V2 = rnorm(5),
                        V3 = rnorm(5),
                        V4 = c("A","B","B","B","B"))


log_transform <- function(test_data){      
rescaling <- function(x){      
log((x - min(x)) / (max(x) - min(x)) + 1)}      
num_var <- sapply(test_data, is.numeric)      
test_data[num_var] <- mutate_each(test_data[num_var], funs(rescaling))      
return(test_data)}



log_transform <- function(test_data){
test_data %>% mutate_if(is.numeric, funs(log(((. - min(.)) /(max(.) - min(.))) + 1)))
}
```


#### Задача


Скачайте файл glacier.csv, если вы ещё этого не сделали, и познакомьтесь с ним поближе. Попробуйте выяснить, о каких ледниках идёт речь в тексте внизу. Можно работать непосредственно в консоли, а можно решать при помощи inline кода, который хорошо подходит в данном случае, если такой текст вы захотите включить в отчёт.

```{r}
glacier <- read_delim("https://raw.githubusercontent.com/tonytonov/Rcourse/master/R%20markdown/demos/glacier.csv", delim = ",", skip = 1, ) %>%
  separate(GEO, c("name", "coord"), sep = " - ")

# Среди шести изучаемых ледников самую короткую историю наблюдений имеет

glacier %>%
  group_by(name) %>%
  summarise(n = min(Ref_Date)) %>% 
  arrange(desc(n))
  
# ледник, для которого медианное значение изменения (переменная Value для Annual Mass Balance) наиболее близко к нулю, называется

glacier %>%
  filter(MEASURE == "Annual mass balance", Value != "..") %>% 
  mutate(Value = as.double(Value)) %>% 
  group_by(name) %>% 
  summarise(n = median(Value)) %>% 
  arrange(desc(n))

# Единственное пропущенное значение содержит история наблюдений ледника

glacier %>%
  filter(Value == "..")
```


#### Задача

В ситуации, когда у нас есть несколько факторов все наши наблюдения разбиваются на столько групп, сколько возможно комбинации уровней факторов. Например, если у нас есть данные, где количественная переменная - размер зарплаты, а два фактора это пол (мужчины и женщины) и национальность (англичане и французы), то два эти фактора разбивают наблюдения на четыре группы:  мужчины французы, женщины француженки, мужчины англичане и женщины англичанки.  В общем число всех возможных групп равняется произведению числа градацией факторов.

И так, ваша задача будет написать функцию descriptive_stats, которая рассчитывает основные описательные статистики в каждой группе наблюдений для описанного выше примера. Функция получает на вход dataframe с тремя переменными salary - значение заработной платы, gender - фактор с двумя градациями (male, female), country - фактор с двумя градациями (England, France).

Функция должна возвращать dataframe с описательными статистиками и количеством NA, рассчитанными в каждой группе: количеств наблюдений, среднее значение, стандартное отклонение, медиана, первый квартиль, третий квартиль, число пропущенных значений.

Воспользуйтесь функцией quantile() для расчета квартилей в данных.

Пример работы функции и формата dataframe c описательными статистиками.

```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/724/salary.csv")
```


```{r}
test_data[,2]


descriptive_stats <- function (dataset){
dataset %>%
  group_by(gender, country) %>%
  summarise(n = n(),
            mean = mean(salary, na.rm = T),
            sd = sd(salary, na.rm = T),
            median = median(salary, na.rm = T),
            first_quartile = quantile(salary, na.rm = T, probs = 0.25),
            third_quartile = quantile(salary, na.rm = T, probs = 0.75),
            na_values = sum(is.na(salary)))
}
```

#### Задача

Напишите функцию, to_factors, которая получает на вход dataframe  с произвольным числом количественных переменных и вектор с номерами колонок, которые нужно перевести в фактор.

Для перевода числовых колонок в фактор будем использовать следующий принцип, если наблюдение больше среднего всей переменной то 1, иначе 0.

```{r}
test <- mtcars[1:4]

# Желаемое состояние

test[c(1, 3)] <- test[c(1, 3)] %>%
  mutate(mpg = as.factor(ifelse(mpg > mean(mpg), 1, 0)),
         disp = as.factor(ifelse(disp > mean(disp), 1, 0)))


# Переводим в функцию при помощи mutate_each() 

to_factors <- function(test_data, factors){
test_data[factors] <- mutate_each(test_data[factors], funs(as.factor(ifelse(. > mean(.), 1, 0))))
return(test_data)
}

to_factors(mtcars[1:4], factors = c(1, 3))

```

#### Задача

Возьмем данные diamonds для работы в этой задаче. Создайте новый dataframe с именем high_price, в котором будут хранится только 10 самых дорогих бриллиантов каждого цвета. Также в итоговом datafrmae должны храниться только две переменные color и price.

```{r}
diamonds %>% 
  group_by(color) %>%
  arrange(desc(price)) %>% 
  slice(1:10) %>% 
  select(color, price)
```

## Data.table

```{r}
library(data.table)

products <- fread("data/products.csv", colClasses = c(price = "double"))


```

#### Задача

Напишите функцию filter.expensive.available, которая принимает на вход products (объект типа data.table) и вектор названий брендов, и возвращает только те строчки, которые соответствуют товарам, цена которых больше или равна 5000 рублей, доступны на складе, и принадлежат одному из переданных брендов.

```{r}
sample.products <- data.table(price = c(10000, 600000, 700000, 1000000),
                              brand = c("a", "b", "c", "d"),
                              available = c(T, T, F, T))

filter.expensive.available <- function(products, brands) {
  products[price/100 > 5000 & available == T][brand %in% brands]
}

filter.expensive.available(sample.products, c("a", "c", "d"))
```

#### Задача

Создайте функцию ordered.short.purchase.data, которая будет принимать purchases, объект data.table, и возвращать таблицу только со столбцами с номером заказа и ID продукта.
Упорядочите результат по убыванию стоимости купленного товара. Возвраты (записи с отрицательным количеством предметов в позиции) надо удалить.

```{r}
sample.purchases <- data.table(price = c(100000, 6000, 7000, 5000000),
                               ordernumber = 1:4,
                               quantity = c(1,2,1,-1),
                               product_id = 1:4)

ordered.short.purchase.data <- function(purchases) {
  purchases[order(-price)][quantity > 0][, list(ordernumber, product_id)]
}

```

#### Задача
Напишите функцию purchases.median.order.price, у которой один аргумент: purchases, и которая возвращает медианную стоимость заказа (число).

Группировку стоит проводить с помощью data.table. Записи с неположительным количеством купленных товаров (возвраты) игнорировать.

Обратите внимание, что одному заказу может соответствовать несколько записей – «позиций» с одинаковым ordernumber, и что при расчете стоимости заказа надо учитывать ситуации, когда пользователь купил несколько товаров одного типа (их количество указано в quantity).


```{r}
sample.purchases <- data.table(price = c(100000, 6000, 7000, 5000000),
                               ordernumber = c(1,2,2,3),
                               quantity = c(1,2,1,-1),
                               product_id = 1:4)


purchases.median.order.price <- function(purchases) {
purchases[quantity > 0][, sum(price * quantity), by = ordernumber][,median(V1)]
}

```

#### Задача

Создайте функцию get.category.ratings, которая будет возвращать суммарный оборот (с учетом скидок) каждой категории , и количество купленных предметов по таблице покупок и таблице принадлежности товара к категории. Если купленный товар принадлежит нескольким категориям, его необходимо учитывать во всех. При решении используйте ключи.


```{r}
product.category <- data.table(product_id = c(1,1,2,2,3),
                               category_id = c(1,2,1,3,3))

purchases <- data.table(product_id = c(1, 2, 3),
                        totalcents = c(100, 200, 300),
                        quantity = c(1, 1, 3))


get.category.ratings <- function(purchases, product.category) {
  setkey(purchases, product_id)
  setkey(product.category, product_id)
f <- merge(product.category, purchases)[, .(totalcents=sum(totalcents)), by = category_id]
g <- merge(product.category, purchases)[, .(quantity=sum(quantity)), keyby = category_id]
f[g, on = "category_id"]
}

get.category.ratings(purchases, product.category)

```

#### Задача

Напишите функцию, которая будет с помощью := добавлять столбец «price.portion», содержащий процент стоимости товара в заказе, с двумя знаками после запятой (нули после запятой не опускать). Проверяться будет возвращаемая из функции таблица. Тип нового столбца - character (строка). Записи с неположительным количеством товаров убрать перед расчётом.

```{r}
sample.purchases <- data.table(price = c(100, 300, 50, 700, 30),
                               ordernumber = c(1,1,1,2,3),
                               quantity = c(1,1,2,1,-1),
                               product_id = 1:5)

sample.purchases[quantity > 0][, price.portion := (quantity * price), by = ordernumber]

sample.purchases[, .(price)]

mark.position.portion <- function(purchases) {
  purchases <- purchases[quantity > 0][, price.portion := (quantity * price) / sum(quantity * price) * 100, by = ordernumber]
 purchases$price.portion <- sapply(purchases[, .(price.portion)], function(x) sprintf("%.2f",x))
  as.data.table(purchases)
 print(purchases)
}

sprintf("%.2f", pi)

sample.purchases$price <-  sapply(sample.purchases[, .(price)], function(x) sprintf("%.2f",x))

str(sample.purchases)

is.data.table(mark.position.portion(sample.purchases))


str(sample.purchases)
is.data.table(sample.purchases)
```



## Грамматика ggplot2, функция qplot

Куча расширений для ggplot <http://www.ggplot2-exts.org/gallery/>

#### Задача

Используя функцию qplot, постройте гистограмму переменной depth из данных diamonds. Сохраните график в переменную depth_hist.

```{r}
qplot(data = diamonds, depth)
```

#### Задача

Постройте диаграмму рассеивания (scatter plot) как в указанном ниже примере, результат сохраните в переменную price_carat_clarity_points.

данные - diamonds
ось x - carat
ось y - price
цвет точек - clarity

```{r}
qplot(x = carat, y = price, data = diamonds, geom = "point", color = clarity)
```

#### Задача

Используя функцию qplot, постройте график плотности переменной x из данных diamonds. Сохраните график в переменную x_density.

```{r}
qplot(x = x, data = diamonds, geom = "density", color = cut)
```

#### Задача

Давайте знакомиться с различными geoms. Документация пакета и весь список с примерами использования доступен здесь. Вообще обратите внимание на документацию ggplot2, очень подробное описание основных возможностей пакета!

Давайте построим график violin plot для переменной price в каждой группе наблюдений по переменной color. Сохраните результа в переменную price_violin.

```{r}
qplot(x = color, y = price, data = diamonds, geom = "violin")
```

## Функция ggplot и различные geoms

#### Задача

Давайте потренируемся комбинировать различные geoms на одном графике. Используя данные mtcars скомбинируем два варианта отображения количественных данных boxplot и violin plot:

```{r}
ggplot(mtcars, aes(factor(am), mpg)) +
  geom_violin() +
  geom_boxplot(width = 0.2)
```

#### Задача

Для закрепления результатов поработаем с вымышленными данными о доходах в нескольких магазинах:

sale - число проданных товаров
shop - номер магазина
date - год, за который велась статистика
season - время года
income - доход магазина
В последующих заданиях мы визуализируем связи между переменным

```{r}
sales = read.csv("https://stepic.org/media/attachments/course/724/sales.csv", encoding = "UTF-8")
str(sales)
```

#### Задача

Отобразите взаимосвязь между доходом (income) и числом продаж (sale), цветом точек указав номер магазина (shop):

```{r}
ggplot(sales, aes(income, sale)) + 
  geom_point(aes(color = shop)) +
  geom_smooth(method = "lm")
```

#### Задача

При помощи функции stat_summary постройте график с доверительными интервалами для демонстрации различий в доходах двух магазинов с учетом времени года:

переменная shop - ось x;
переменная income - ось y;
переменная season - цвет;
geom pointrange.
Сохраните график в переменную my_plot, дополнив предложенный код. Обратите внимание, что доверительные интервалы не накладываются друг на друга! 

```{r}
ggplot(sales, aes(shop, income, color = season)) +
stat_summary(fun.data = mean_cl_boot, geom = "pointrange", position = position_dodge(width = .5))
```

#### Задача

Теперь давайте отобразим на графике различия в продажах (переменная sale), в зависимости от:

года (date) - ось x;
и номера магазина (shop) - цвет.
Дополните предложенный код, чтобы получился график как в примере ниже. Используйте функцию mean_cl_boot для построения доверительных интервалов.Вам также понадобится использовать три geoms: errorbar, point, line. Используйте их прямо внутри функции stat_summary(). 

Данные уже сохранены в переменную sales. Нажмите, начать решение, чтобы увидеть заготовку кода.

```{r}
ggplot(sales, aes(date, sale, color = shop))+
stat_summary(fun.data = mean_cl_boot, geom = "errorbar", position = position_dodge(width = .5)) + # добавим стандартную ошибку
stat_summary(fun.data = mean_cl_boot, geom = "point", position = position_dodge(width = .5)) + # добавим точки
stat_summary(fun.data = mean_cl_boot, geom = "line", position = position_dodge(width = .5))
```

## Facet - способы группировки данных на графике

#### Задача

Потренируемся с разбиением графика на подгруппы! Используя facet_grid постройте следующий график и сохраните его в переменную mpg_facet.

```{r}
mtcars <- mutate(mtcars, 
                 am = factor(am, labels = c("A", "M")), 
                 vs = factor(vs, labels = c("V", "S")))

ggplot(mtcars, aes(mpg)) +
  geom_dotplot() +
  facet_grid(am ~ vs)
```

#### Задача

Используя данные iris, постройте график плотности для переменной Sepal.Length. Разбейте график на части по переменной Species при помощи facet_wrap. Результат сохраните в переменную sl_wrap.


```{r}
ggplot(iris, aes(Sepal.Length)) +
  geom_density() +
  facet_wrap(~Species)
```

#### Задача

Используя данные Iris, постройте график, иллюстрирующий взаимосвязь переменных Sepal.Length и Sepal.Width внутри каждого вида (переменной Species), при помощи facet_wrap().

В этом задании вам потребуется использовать два geom:

geom_point - для отображения отдельных наблюдений,
geom_smooth - для добавления сглаживания.

```{r}
ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~Species)
```

#### Задача

«Отряд самоубийц» уже посмотрели? Пишите в комментариях, как вам! У меня вот смешанные чувства вызвала эта команда суперзлодеев. Собрали самых интересных персонажей вселенной и отправили их в одну локацию весь фильм ходить, перебрасываясь односложными фразами. Джокера совсем не показали, все так ждали Джареда Лето в этой роли, а он появился, считайте, в двух эпизодах! К чему это я? Поговорим о кино!

Вы можете скачать данные myMovieData (жмите на ссылку), в которых представлена различная информация о голливудских фильмах с 2002 по 2005: тип жанр, бюджет и год выхода на экраны. Давайте построим следующий график, чтобы выяснить есть ли различия в бюджетах фильмов разного жанра из года в год. Cохраните результат в переменную my_plot.

ось x - переменная Type
ocь y - переменная Budget
facet - переменная Year (используйте facet_grid)

```{r}
myMovieData <- read_csv("data/myMovieData.csv")

ggplot(myMovieData, aes(Type, Budget)) +
  geom_boxplot() +
  facet_grid(~Year) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Scale и Theme: оси, легенда, внешний вид графика

#### Задача

В этом задании мы построим график используя данные Iris. Наша цель отобразить взаимосвязь переменных Sepal.Length (ось X) и Petal.Length (ось Y) внутри трех групп по переменной Species. Для этого постройте scaterplot, отобразите цветом значения переменной Species и добавьте линейное сглаживание в каждой группе.

Далее от вас потребуется привести график к более завершенному виду. Мы переведем на русский название осей, название легенды и ее расшифровку:

Ось X - "Длина чашелистика".
Ось Y - "Длина лепестка".
Название легенды - "Вид цветка".
Расшифровка легенды: "Ирис щетинистый", "Ирис разноцветный", "Ирис виргинский".
Также мы чуть измени отображение значений по осям. 

Значения по оси X должны начинаться с 4 и заканчиваться на 8 с шагом в единицу.
Значения по оси Y должны начинаться с 1 и заканчиваться на 7 с шагом в единицу.

Дополните шаблон графика чтобы получился следующий итоговый результат:

```{r}
ggplot(iris, aes(Sepal.Length, Petal.Length, color = Species))+
  geom_point() +
  geom_smooth(method = "lm")+
  scale_color_discrete(name = "Вид цветка",
                       labels = c("Ирис щетинистый", "Ирис разноцветный", "Ирис виргинский"))+
  scale_x_continuous(name = "Длина чашелистика",
                     limits = c(4, 8),
                     breaks = c(1, seq(4, 8, 1))) + 
  scale_y_continuous(name = "Длина лепестка",
                     limits = c(1, 7),
                     breaks = c(1, seq(1, 7, 1)))
```

## Динамическая визуализация с plotly

Plotly не ограничивается функцией ggplotly. У него есть свои конструкции для визуализации, а именно -  plot_ly.

Идейно она похожа на ggplot(…). В неё тоже можно передать данные, а также информацию об эстетиках, таких, как имя столбца с цветом, группировку, и так далее. Отличием является то, что в plotly мы указываем тип примитивов с помощью параметра type, и объединяем их в один объект с помощью "%>%", а в ggplot'e для каждого типа была своя функция-конструктор, и объединение происходило через "+".

Взглянем на сигнатуру plot_ly:

```{r}
purchases <- fread("data/purchases.csv")
```


```{r, eval = FALSE}
plot_ly(data = data.frame(), ..., type = "scatter", group, color, colors,
  symbol, symbols, size, width = NULL, height = NULL, inherit = TRUE,
  evaluate = FALSE)
```



data – dataframe, источник данных
group – вектор/столбец для группировки данных
color – вектор/столбец с цветом
symbol – вектор/столбец, отвечающий за форму точки
size – вектор/столбец, отвечающий за размер точек
type – тип графика.
… - параметры, релевантные для данного type (x, y, z, …)

Одним вызовом plot_ly можно нарисовать график, если указать все необходимые параметры (по аналогии с qplot):

```{r}
plot_ly(mtcars, x = mpg, y = ~disp, text = rownames(mtcars), mode="markers", type="scatter")
```


NB! Если у вас старая версия plot_ly, то перед аргументами не надо писать символ ~:

Параметр text задает текст, появляющийся, при наведении курсора на точку. Mode="markers" говорит, что на графике будут появляться только точки (попробуйте "markers+text").

Дополнительные объекты на графике (add_trace) и настройки (layout) добавляются через %>%. Сигнатура add_trace совпадает с сигнатурой plot_ly, за исключением параметра data (у add_trace первым параметром идет объект plotly, для поддержки оператора %>%). Давайте на основе датасета economics, содержащей информацию об экономическом состоянии, сделаем график, отображающий рост медианного времени безработицы, и сглаженную версию этой кривой:


```{r}
smoothed.vector <- fitted(loess(uempmed ~ as.numeric(date), economics, span = 0.2))
plot_ly(economics, x = ~date, y = ~uempmed, type = "scatter", showlegend = FALSE, mode="lines+markers") %>%
    add_trace(x = date, y = smoothed.vector)
```

    
Вектор smoothed.vector содержит сглаженные через loess значения медианного времени безработицы. При передаче его в add_trace добавляется новая сглаженная кривая. Это же можно добиться с помощью ggplot'a:

```{r}
smoothed.vector <- fitted(loess(uempmed ~ as.numeric(date), economics, span = 0.2))
my.plot <- ggplot(economics) + 
  geom_line(aes(x = date, y = uempmed), color = "red") + 
  geom_line(aes(x = date, y = smoothed.vector), color = "blue")
ggplotly(my.plot)
```



Графики можно нарисовать и в plotly, и в ggplot с последующей конвертацией через ggplotly. Второй способ обладает большей гибкостью, так как синтаксис ggplot несколько мощнее. Например, такой график через plotly элегантно уже не сделать:

```{r}
purchases <- fread("purchases.csv")
purchases[, log.tc := log(purchases$totalcents)]
purchases[, normal.approx.prob := dnorm(log.tc, mean(log.tc), sd(log.tc))]

ggplotly(ggplot(purchases, aes(log.tc)) + 
  geom_histogram(aes(y=..count.. / max(..count..)), fill="white", color="blue", binwidth = 0.2) + 
  geom_line(aes(log.tc, normal.approx.prob / max(normal.approx.prob)), color="red"))

```


Поэтому мы не будем подробно останавливаться на scatterplot’ах, гистограммах и других простых вещах. Подробную документацию по всевозможным параметрам можно найти тут. 3D-визуализация – это, наверное, наиболее зрелищный способ визуализации. Она актуальна только для интерактивной графики: трехмерную модель можно поворачивать, рассматривая с разных сторон и ракурсов. Поэтому в деталях разберем типы "surface" и "mesh3d".

Тип графика "surface" предполагает, что функции передадут аргумент z, который будет содержать так называемую «карту высот»: матрицу, где в каждой ячейке содержится замер высоты поверхности в данной точке ячейки. По этой матрице plot_ly строит трехмерную поверхность:

plot_ly(z = ~volcano, type="surface")
- нарисует вулкан. На одном полотне можно отображать несколько трехмерных объектов.

Тип "mesh3d" отрисовывает набор треугольников (полигонов) по координатам в пространстве. Ему необходимо передать координаты точек через параметры x, y, z, и метод, согласно которому по этим координатам будут строиться треугольники. Алгоритм задается через параметр alphahull: -1 - триангуляция Делоне (по умолчанию), 0 - выпуклая оболочка, 1 - альфа-форма. Нарисуем выпуклую оболочку по случайным точкам:

```{r}
mesh <- data.table(
  x = rnorm(40),
  y = rnorm(40),
  z = rnorm(40)
)
plot_ly(mesh, type="mesh3d", x = ~x, y = ~y, z = ~z, alphahull = 0)
```


Вместо использования алгоритма триангуляции можно явно задать индексы i, j, k, которые будут означать номера (индексы) точек в исходном массиве координат, которые станут вершинами полигона (0-based индексы; рекомендуемый авторами пакета способ). То есть, у первого треугольника вершинами будут точки с индексами i[1], j[1], k[1], у второго - i[2], j[2], k[2]. Нарисуем трапецию.
Сначала зададим координаты её углов:

```{r}
points <- data.table(
  x = c(0.2, 0.8, 0, 1),
  y = c(0, 0, 1, 1),
  z = c(0, 0, 0, 0)
)
```

А затем - индексы полигонов. Трапецию можно нарисовать как два треугольника:

```{r}
i.s <- c(0, 2)
j.s <- c(1, 1)
k.s <- c(2, 3)
plot_ly(points, 
        x = ~x, y = ~y, z = ~z, 
        i = ~i.s, j = ~j.s, k = ~k.s,
        type = "mesh3d")
```

#### Задача

Датасет (ссылка) содержит информацию о полигонах трехмерной модели чайника. Три столбца таблицы задают координаты (x,y,z), а тройки строк задают треугольники (т.е. строки 1,2,3 - первый треугольник, 4,5,6 - второй, и так далее).

Напишите функцию, которая будет принимать data.table с этими данными, и возвращать объект plotly с трехмерной моделью. Следует воспользоваться установкой индексов i,j,k.

NB! не стоит хардкодить количестве строк в датасете. Решение не должно зависеть от этого параметра.

```{r}
teapot <- read_delim("data/teapot.csv", ";", escape_double = FALSE, trim_ws = TRUE)
i<- seq(0,length(teapot$z)-1,3)
j<- seq(1,length(teapot$z),3)
k<- seq(2,length(teapot$z),3)

plot_ly(teapot, type="mesh3d", 
        x = ~x, y = ~y, z = ~z,
        i = ~i, j = ~j, k = ~k,
        alphahull = 0)

teapot$z
teapot[[1]]


make.fancy.teapot <- function(teapot.coords) {
i<- seq(0,length(teapot.coords[[1]])-1,3)
j<- seq(1,length(teapot.coords[[1]]),3)
k<- seq(2,length(teapot.coords[[1]]),3)

plot_ly(teapot.coords, type="mesh3d", 
        x = ~x, y = ~y, z = ~z,
        i = ~i, j = ~j, k = ~k,
        alphahull = 0)
}

make.fancy.teapot <- function(teapot.coords) {
    i = seq(0,nrow(teapot.coords)-1,3)
    plot_ly(teapot.coords, x = x, y = y,   z = z, 
                           i = i, j = i+1, k = i+2, type = "mesh3d")
}

make.fancy.teapot(teapot)
```


